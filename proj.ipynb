{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk import bigrams,trigrams\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords to prevent bad recommendations like conjunctions and prepositions\n",
    "stopwords=['the','as','he','she','they','a','into','of','with','this','in','and','for','is','was','an','on','at','to']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1: Corpus Creation\n",
    "def extract_body_text ( filename :str ) -> str :#Code that was given\n",
    "    file = open ( filename )\n",
    "    paper_content = json.load ( file )\n",
    "    body_text = \"\"\n",
    "    if 'body_text' in paper_content :\n",
    "      for bt in paper_content ['body_text']:\n",
    "        body_text = body_text + bt['text']\n",
    "    return ( body_text + '\\n'). lower ()\n",
    "\n",
    "\n",
    "for name in glob.glob('pdf_json/*'):#writing files\n",
    "    body = extract_body_text(name)\n",
    "    f = open('textfiles/'+name[8:-4]+'txt','w',encoding='utf-8')\n",
    "    f.write(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2- Preprocessing step\n",
    "parent_list = os.listdir(\"textfiles\")\n",
    "for child in parent_list:\n",
    "      f=open('textfiles/'+child,'r',encoding='utf-8')\n",
    "      pptext = f.read()\n",
    "      pptext = re.sub(r'[,\\d+%?]',' ',pptext)\n",
    "      pptext = re.sub(r'([^a-zA-Z\\s\\.\\-]|[\\[\\]])','',pptext)#removing foreign characters and brackets\n",
    "      pptext = re.sub(r'-','- ',pptext)\n",
    "      pptext = pptext.lower().split()# makes a list of text into word sequence\n",
    "      pptext = ' '.join(pptext)\n",
    "      w = open('tfpp/'+child,'w',encoding='utf-8')\n",
    "      w.write(pptext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab length is: 1163688\n"
     ]
    }
   ],
   "source": [
    "#Q3- Finding Vocab Count\n",
    "parent_list = os.listdir('tfpp')\n",
    "vocab=[]\n",
    "for child in parent_list:\n",
    "    f=open('tfpp/'+child,'r',encoding='utf-8')\n",
    "    vocab.extend(re.sub(r'\\.',' ',f.read()).split())#Add all words to a list, then use hashtable to form a set\n",
    "vocab=Counter(vocab)\n",
    "print('Vocab length is: '+str(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [112], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m         bigram_freq\u001b[39m.\u001b[39mupdate(bigrams(words))\u001b[39m# Frequencies are kept, Probabilities are calculated only when required\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mbigram_model.pickle\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\u001b[39m#Using pickle to store the model\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     pickle\u001b[39m.\u001b[39mdump(bigram_freq,f)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "#Q-4- Building Bigram Model\n",
    "parent_list=os.listdir('tfpp')\n",
    "bigram_freq=Counter()\n",
    "\n",
    "for child in parent_list:\n",
    "    f=open('tfpp/'+child,'r')\n",
    "    sents=f.read().split('.')\n",
    "    for sentence in sents:\n",
    "        words=sentence.split()\n",
    "        bigram_freq.update(bigrams(words))# Frequencies are kept, Probabilities are calculated only when required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building Trigram Model\n",
    "trigram_freq=Counter()\n",
    "for child in parent_list:\n",
    "    f=open('tfpp/'+child,'r')\n",
    "    sents=f.read().split('.')\n",
    "    for sentence in sents:\n",
    "        words=sentence.split()\n",
    "        trigram_freq.update(trigrams(words))#Bigrams and Trigrams are functions from nltk library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trigram_model.pickle','wb') as f:#Using pickle to store the model\n",
    "    pickle.dump(trigram_freq,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bigram_model.pickle','wb') as f:#Using pickle to store the model\n",
    "    pickle.dump(bigram_freq,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bigram_model.pickle','rb') as f:\n",
    "    bigram_freq = pickle.load(f)\n",
    "with open('trigram_model.pickle','rb') as f:\n",
    "    trigram_freq = pickle.load(f)\n",
    "def prob_words_bi(atuple):#P(w(i+1)|w(i))\n",
    "    if atuple[1] not in stopwords:#increasing quality of recommendations\n",
    "        return (bigram_freq[atuple] + 1) / (vocab[atuple[0]] + len(vocab))#Laplacian smoothing\n",
    "    else:\n",
    "        return 1/(vocab[atuple[0]] + len(vocab))\n",
    "def pred_next_bigram(word):#Predicts Next Word\n",
    "    candidates=[i for i in bigram_freq if i[0]==word]\n",
    "    mydict={}#Use dictionary to store all candidate tuples\n",
    "    for i in candidates:\n",
    "        mydict[i] = prob_words_bi(i)\n",
    "    return max(mydict, key= lambda x: mydict[x])[1]\n",
    "def prob_words_tri(atuple):#P(w(i+2)|w(i+1),w(i))\n",
    "    if atuple[2] not in stopwords:\n",
    "        return (trigram_freq[atuple]+1) / (bigram_freq[atuple[:2]]+len(vocab))\n",
    "    else:\n",
    "        return 1/(bigram_freq[atuple[:2]]+len(vocab))\n",
    "def pred_next_trigram(atuple):\n",
    "    candidates=[i for i in trigram_freq if i[:2]==atuple]\n",
    "    mydict={}\n",
    "    for i in candidates:\n",
    "        mydict[i] = prob_words_tri(i)\n",
    "    return max(mydict, key = lambda x: mydict[x])[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ten_probable_bi(word):#10 most probable next words for word, bigram\n",
    "    candidates=[i for i in bigram_freq if i[0]==word]\n",
    "    mydict={}\n",
    "    for i in candidates:\n",
    "        mydict[i]=prob_words_bi(i)\n",
    "    return sorted(mydict, key= lambda x: mydict[x],reverse=True)[:10]\n",
    "def ten_probable_tri(atuple):#for trigrams\n",
    "    candidates=[i for i in trigram_freq if i[:2]==atuple]\n",
    "    mydict={}\n",
    "    for i in candidates:\n",
    "        mydict[i]=prob_words_tri(i)\n",
    "    return sorted(mydict, key= lambda x: mydict[x],reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity_bi(sentence):#perplexity for bigrams \n",
    "    words=sentence.split()\n",
    "    n=len(words)\n",
    "    words=list(bigrams(words))\n",
    "    ppscore=1\n",
    "    for i in words:\n",
    "        ppscore *= prob_words_bi(i)\n",
    "    return (1/ppscore)**(1/n)\n",
    "def perplexity_tri(sentence):#perplexity for trigrams\n",
    "    words=sentence.split()\n",
    "    n=len(words)\n",
    "    words=list(trigrams(words))\n",
    "    ppscore = 1\n",
    "    for i in words:\n",
    "        ppscore *= prob_words_tri(i)\n",
    "    return (1/ppscore)**(1/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_missing_text_bi(sentence):#predicts missing text wherever '%%' is used, for bigrams\n",
    "    words=sentence.split()\n",
    "    for i in range(len(words)):\n",
    "        if words[i]==r'%%':\n",
    "            words[i]=pred_next_bigram(words[i-1])\n",
    "    return ' '.join(words)\n",
    "def pred_missing_text_tri(sentence):#for trigrams\n",
    "    words=sentence.split()\n",
    "    for i in range(len(words)):\n",
    "        if words[i]==r'%%':\n",
    "            words[i]=pred_next_trigram((words[i-2],words[i-1]))\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: all houses were not ventilated\n",
      "Sentence 2: it aims to develop an integrated dna to reach mmps exposed to malaria with prevention diagnosis and treatment group by involving non- health care stakeholders from provincial to community level\n",
      "Sentence 3: this is because engineers do not work in addition but rather as a team\n"
     ]
    }
   ],
   "source": [
    "#Q5 find missing text: bigrams\n",
    "sen1=r'all houses were %% ventilated'\n",
    "sen2=r'it aims to develop an integrated %% to reach mmps exposed to malaria with prevention diagnosis and treatment %% by involving non- health %% stakeholders from provincial to community level'\n",
    "sen3=r'this is because engineers do not work in %% but rather as a team'\n",
    "print('Sentence 1: '+pred_missing_text_bi(sen1))\n",
    "print('Sentence 2: '+pred_missing_text_bi(sen2))\n",
    "print('Sentence 3: '+pred_missing_text_bi(sen3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: all houses were made ventilated\n",
      "Sentence 2: it aims to develop an integrated approach to reach mmps exposed to malaria with prevention diagnosis and treatment strategies by involving non- health care stakeholders from provincial to community level\n",
      "Sentence 3: this is because engineers do not work in ensuring but rather as a team\n"
     ]
    }
   ],
   "source": [
    "#Q5 find missing text: trigrams\n",
    "print('Sentence 1: '+pred_missing_text_tri(sen1))\n",
    "print('Sentence 2: '+pred_missing_text_tri(sen2))\n",
    "print('Sentence 3: '+pred_missing_text_tri(sen3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: [('were', 'not'), ('were', 'used'), ('were', 'also'), ('were', 'performed'), ('were', 'found'), ('were', 'collected'), ('were', 'obtained'), ('were', 'observed'), ('were', 'identified'), ('were', 'detected')]\n",
      "\n",
      "Sentence 2 word 1: [('integrated', 'dna'), ('integrated', 'approach'), ('integrated', 'care'), ('integrated', 'moving'), ('integrated', 'system'), ('integrated', 'model'), ('integrated', 'analysis'), ('integrated', 'health'), ('integrated', 'management'), ('integrated', 'within')]\n",
      "\n",
      "sentence 2 word 2: [('treatment', 'group'), ('treatment', 'options'), ('treatment', 'or'), ('treatment', 'groups'), ('treatment', 'strategies'), ('treatment', 'were'), ('treatment', 'should'), ('treatment', 'has'), ('treatment', 'may'), ('treatment', 'option')]\n",
      "\n",
      "sentence 2 word 3: [('health', 'care'), ('health', 'organization'), ('health', 'system'), ('health', 'services'), ('health', 'systems'), ('health', 'emergency'), ('health', 'professionals'), ('health', 'authorities'), ('health', 'status'), ('health', 'outcomes')]\n",
      "\n",
      "Sentence 3: [('in', 'addition'), ('in', 'patients'), ('in', 'our'), ('in', 'order'), ('in', 'which'), ('in', 'vitro'), ('in', 'table'), ('in', 'all'), ('in', 'both'), ('in', 'perpetuity')]\n"
     ]
    }
   ],
   "source": [
    "#Q5: top 10 words: bigrams\n",
    "print('Sentence 1: '+str(ten_probable_bi('were'))+'\\n')\n",
    "print('Sentence 2 word 1: '+str(ten_probable_bi('integrated'))+'\\n')\n",
    "print('sentence 2 word 2: '+str(ten_probable_bi('treatment'))+'\\n')\n",
    "print(\"sentence 2 word 3: \"+str(ten_probable_bi('health'))+'\\n')\n",
    "print('Sentence 3: '+str(ten_probable_bi('in')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: [('houses', 'were', 'made'), ('houses', 'were', 'constructed'), ('houses', 'were', 'built'), ('houses', 'were', 'investigated'), ('houses', 'were', 'malaria'), ('houses', 'were', 'contacted'), ('houses', 'were', 'tested'), ('houses', 'were', 'then'), ('houses', 'were', 'no'), ('houses', 'were', 'not')]\n",
      "\n",
      "Sentence 2 word 1: [('an', 'integrated', 'approach'), ('an', 'integrated', 'system'), ('an', 'integrated', 'model'), ('an', 'integrated', 'analysis'), ('an', 'integrated', 'framework'), ('an', 'integrated', 'health'), ('an', 'integrated', 'platform'), ('an', 'integrated', 'part'), ('an', 'integrated', 'view'), ('an', 'integrated', 'one')]\n",
      "\n",
      "sentence 2 word 2: [('and', 'treatment', 'strategies'), ('and', 'treatment', 'options'), ('and', 'treatment', 'are'), ('and', 'treatment', 'plan'), ('and', 'treatment', 'protocol'), ('and', 'treatment', 'guidelines'), ('and', 'treatment', 'protocols'), ('and', 'treatment', 'services'), ('and', 'treatment', 'program'), ('and', 'treatment', 'were')]\n",
      "\n",
      "sentence 2 word 3: [('non-', 'health', 'care'), ('non-', 'health', 'workers'), ('non-', 'health', 'sectors'), ('non-', 'health', 'sector'), ('non-', 'health', 'consequences'), ('non-', 'health', 'numeracy'), ('non-', 'health', 'professionals'), ('non-', 'health', 'related'), ('non-', 'health', 'benefits'), ('non-', 'health', 'gdp')]\n",
      "\n",
      "Sentence 3: [('work', 'in', 'ensuring'), ('work', 'in', 'progress'), ('work', 'in', 'our'), ('work', 'in', 'concert'), ('work', 'in', 'other'), ('work', 'in', 'order'), ('work', 'in', 'which'), ('work', 'in', 'close'), ('work', 'in', 'these'), ('work', 'in', 'sect')]\n"
     ]
    }
   ],
   "source": [
    "#Q5: top 10 words: trigrams\n",
    "print('Sentence 1: '+str(ten_probable_tri(('houses','were')))+'\\n')\n",
    "print('Sentence 2 word 1: '+str(ten_probable_tri(('an','integrated')))+'\\n')\n",
    "print('sentence 2 word 2: '+str(ten_probable_tri(('and','treatment')))+'\\n')\n",
    "print(\"sentence 2 word 3: \"+str(ten_probable_tri(('non-','health')))+'\\n')\n",
    "print('Sentence 3: '+str(ten_probable_tri(('work','in'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1 perplexity score: 3585.683485011883\n",
      "Sentence 2 perplexity score: 9957.038437205303\n",
      "Sentence 3 perplexity score: 58126.10000276572\n"
     ]
    }
   ],
   "source": [
    "#Q6: perplexity score:bigrams\n",
    "sena='it appears that the overall code stroke volume has decreased since the covid- pandemic'\n",
    "senb='half a century ago hypertension was not treatable'\n",
    "senc='sarahs tv is broadcasting an advert for private healthcare'\n",
    "print('Sentence 1 perplexity score: '+ str(perplexity_bi(sena)))\n",
    "print('Sentence 2 perplexity score: '+ str(perplexity_bi(senb)))\n",
    "print('Sentence 3 perplexity score: '+str(perplexity_bi(senc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1 perplexity score: 9921.30537874307\n",
      "Sentence 2 perplexity score: 5682.088884112009\n",
      "Sentence 3 perplexity score: 35534.18736702119\n"
     ]
    }
   ],
   "source": [
    "#Q6: perplexity score: trigrams\n",
    "print('Sentence 1 perplexity score: '+ str(perplexity_tri(sena)))\n",
    "print('Sentence 2 perplexity score: '+ str(perplexity_tri(senb)))\n",
    "print('Sentence 3 perplexity score: '+str(perplexity_tri(senc)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pybreh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "898642a1069e0584d33780048ab97813ab713ec4cbeada735284f551cbefe30f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
